{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliothèque standard pour ouvrir des URL(avec la fonction urlopen) et lire le contenu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from urllib.request import urlopen \n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "\n",
    "print(html.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "Beautiful Soup est une bibliothèque Python qui permet d'extraire des données de fichiers HTML et XML. Elle propose des idiomes Python pour itérer, rechercher et modifier l'arbre de syntaxe. \n",
    "\n",
    "## Objectif et Cas d'utilisation\n",
    "\n",
    "Il y'a deux cas d'utilisation que l'on peut noter :\n",
    "\n",
    "Web Scraping : BeautifulSoup est couramment utilisé pour le scraping web, qui consiste à extraire des données de sites Internet.\n",
    "Data Mining : Elle permet aux analystes et développeurs d'extraire des informations et de les comprendre en les convertissant en un format facilement manipulable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caractéristiques Principales\n",
    "Il possède plusieurs caractéristiques à savoir la :\n",
    "\n",
    "- Robustesse : Elle est conçue pour gérer le désordre du web réel, qui comprend souvent du HTML incorrect.\n",
    "Flexibilité : BeautifulSoup vous permet de spécifier un analyseur, ce qui signifie que vous pouvez changer votre approche en fonction des exigences spécifiques du HTML avec lequel vous travaillez.\n",
    "- Facilité d'Utilisation : La bibliothèque est connue pour son API simple et les méthodes Pythoniques de navigation, de recherche et de modification de l'arborescence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodes et Propriétés Communes\n",
    "Les propriétés et les méthodes qu'il faut connaitre sur ce module sont :\n",
    "\n",
    "- Navigation dans l'Arbre : Des méthodes telles que .parent, .contents, .next_sibling, .prev_sibling permettent la navigation dans l'arborescence.\n",
    "- Recherche dans l'Arbre : .find(), .find_all(), .select(), et d'autres vous permettent de sélectionner des parties du document en utilisant des noms de balises ou des sélecteurs CSS.\n",
    "Modification de l'Arbre : Vous pouvez facilement éditer et modifier l'arbre en ajoutant, modifiant ou supprimant des balises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "L'installation est facile, il suffit de faire un :\n",
    "- $ pip install beautifulsoup4 or\n",
    "- $ pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de continuer, il est conseiller de créer avant de continuer un environnement python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un environnement python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gestion des bibliothèques Python avec les environnements virtuels est essentielle si vous travaillez sur plusieurs projets Python, si vous avez besoin de regrouper facilement des projets avec toutes les bibliothèques associées, ou si vous vous inquiétez des conflits potentiels entre les bibliothèques installées. Un environnement virtuel permet de garder chaque projet séparé et facile à gérer.\n",
    "\n",
    "Sans environnement virtuel, l'installation d'une bibliothèque Python est globale, nécessitant souvent des privilèges d'administrateur et rendant la bibliothèque disponible pour tous les utilisateurs et projets sur la machine. Heureusement, créer un environnement virtuel env( qui est le dossier du nouvel environnement) est simple sur linux :\n",
    "\n",
    "- $ python -m venv env\n",
    "- cd env\n",
    "- $ source env/bin/activate\n",
    "\n",
    "Pour plus d'information, aller sur le site [openClassrom](https://openclassrooms.com/fr/courses/6951236-mettez-en-place-votre-environnement-python/7014018-creez-votre-premier-environnement-virtuel).\n",
    "\n",
    "Après avoir créer votre environnement, vous pouvez télécharger tous vos modules à l'intérieur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application du module BeautifulSoup\n",
    "\n",
    "Pour une première application, modifions légèrement le premier code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet bs précédemment crée à cette structure :\n",
    "\n",
    "- html → `<html><head>...</head><body>...</body></html>`\n",
    "  - head → `<head><title>A Useful Page<title></head>`\n",
    "    - title → `<title>A Useful Page</title>`\n",
    "  - body → `<body><h1>An Int...</h1><div>Lorem ip...</div></body>`\n",
    "    - h1 → `<h1>An Interesting Title</h1>`\n",
    "    - div → `<div>Lorem Ipsum dolor...</div>`\n",
    "\n",
    "Un objet bs a deux paramètres :\n",
    "- un premier paramètre qui est le contenu html\n",
    "- un deuxième paramètre qui est le type de parseur à utiliser.\n",
    "\n",
    "En Python, html.parser est le parser intégré par défaut qui peut analyser les documents HTML sans nécessiter de modules externes supplémentaires. Cependant, un inconvénient notable de html.parser est sa vitesse relative plus lente par rapport à d'autres parsers disponibles. Des alternatives comme lxml et html5lib offrent une vitesse de traitement supérieure et sont également plus tolérantes face à du code HTML mal formé, c'est-à-dire qu'ils peuvent mieux gérer et interpréter les erreurs dans le balisage HTML, offrant ainsi une meilleure résilience lors de l'analyse de pages web qui ne suivent pas les standards du HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour continuer, nous allons d'abord voir comment gérer les exceptions.\n",
    "\n",
    "## Gestion des Exceptions\n",
    "\n",
    "- $ html = urlopen('http://www.pythonscraping.com/pages/page1.html') Ce code peut conduire à deux sortes d'erreurs :\n",
    "  - La page n'est pas trouvé sur le serveur.\n",
    "  - Le serveur n'est pas trouvé sur le réseau.\n",
    "\n",
    "Dans ce cas, nous aurons automatiquement des erreurs. Pour éviter cela, nous allons utiliser la gestion des exceptions.\n",
    "Voici un exemple de code qui permet de gérer les exceptions :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "try:\n",
    " html = urlopen('https://pythonscrapingthisurldoesnotexist.com')\n",
    "except HTTPError as e:\n",
    " print(e)\n",
    "except URLError as e:\n",
    " print('The server could not be found!')\n",
    "else:\n",
    " print('It Worked!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
